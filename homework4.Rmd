---
title: "homework4"
author: "James(Changhwan) Han (3923257)"
date: "4/23/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning = FALSE}
library(tidymodels)
library(tidyverse)
library(rlang)
library(knitr)
library(discrim)
library(klaR)
tidymodels_prefer()
```

```{r}
titanic <- read_csv("titanic.csv")

titanic$survived <- factor(titanic$survived, levels = c('Yes', 'No'))
titanic$pclass <- factor(titanic$pclass)

levels(titanic$survived)
```
# Question 1
```{r}
set.seed(1014)

titanic_split <- initial_split(titanic, prop = 0.80,
                               strata = survived)
titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)
dim(titanic_test)
dim(titanic_train)

titanic_recipe <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare,
                         data = titanic_train) %>%
  step_impute_linear(age) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(terms = ~ starts_with("sex"):fare + age:fare)
```
1. Number of observations? train data: 712, test data: 179

# Question 2
```{r}
titanic_folds <- vfold_cv(titanic_train, v = 10)
titanic_folds
```

# Question 3
- What is k-fold cross-validation?
- Why we use it?
- If we used the entire training set, what resampling method would that be?

k-fold cross-validation is a resampling method used to evaluate on a limited data sample. In k-fold cross-validation we split dataset into a K number of folds where each fold is used as a testing set. Using k-fold cross-validation is useful when there is limited observations in a dataset. The aim is to find a best parameter that reduces prediction error.

# Question 4
Set up workflows for 3 models: 

- A logistic regression with the glm engine
- A LDA with the MASS engine
- A QDA with the MASS engine
```{r}
#Recipe
titanic_recipe = recipe(survived ~ pclass + sex + age + sib_sp + parch + fare, 
                        data = titanic_train) %>%
        step_impute_linear(age) %>%
        step_dummy(all_nominal_predictors()) %>%
        step_interact(terms = ~ starts_with("sex"):fare + age:fare)

#Logistic
log_reg = logistic_reg() %>% 
        set_engine("glm") %>% 
        set_mode("classification")

log_wkflow = workflow() %>% 
        add_model(log_reg) %>% 
        add_recipe(titanic_recipe)

log_fit = fit(log_wkflow, titanic_train)

#LDA
lda_mod = discrim_linear() %>%
        set_mode("classification") %>%
        set_engine("MASS")

lda_wkflow = workflow() %>% 
        add_model(lda_mod) %>% 
        add_recipe(titanic_recipe)

lda_fit = fit(lda_wkflow, titanic_train)


#QDA
qda_mod = discrim_quad() %>% 
        set_mode("classification") %>% 
        set_engine("MASS")

qda_wkflow = workflow() %>% 
        add_model(qda_mod) %>% 
        add_recipe(titanic_recipe)

qda_fit = fit(qda_wkflow, titanic_train)
```
10 folds for each 3 models add up to 30 folds total.

# Question 5
```{r, eval = FALSE}
log_res = log_wkflow %>% 
        fit_resamples(resamples = titanic_folds, 
                      metrics = metric_set(recall, precision, 
                                           accuracy, sens, spec, roc_auc),
                      control = control_resamples(save_pred = TRUE)) 

lda_res = lda_wkflow %>%
        fit_resamples(resamples = titanic_folds,
                      metrics = metric_set(recall, precision, 
                                           accuracy, sens, spec, roc_auc),
                      control = control_resamples(save_pred = TRUE))

qda_res = qda_wkflow %>%
        fit_resamples(resamples = titanic_folds,
                      metrics = metric_set(recall, precision, 
                                           accuracy, sens, spec, roc_auc),
                      control = control_resamples(save_pred = TRUE))
```

## Question 6
```{r}
# collect_metrics(log_res)
# collect_metrics(lda_res)
# collect_metrics(qda_res)
```
The logistic model had the best performance. The mean accuracy is at 0.81 and std_err is at 0.01. Assuming from the low standard error and it's mean, the accuracy of folds is all near the mean, 0.81.

## Question 7
```{r}
log_fit = fit(log_wkflow, titanic_train)
```

## Question 8
```{r}
# predict(log_fit, new_data = titanic_test, type = "prob")
# 
# log_acc = augment(log_fit, new_data = titanic_test) %>%
#   accuracy(truth = survived, estimate = .pred_class)
# 
# log_acc

bound_train_data = bind_cols(predict(log_fit, new_data = titanic_train, type = "class"),
                             predict(lda_fit, new_data = titanic_train, type = "class"),
                             predict(qda_fit, new_data = titanic_train, type = "class"))
                             

colnames(bound_train_data) = c("Log fit", "LDA fit", "QDA fit", "True")

log_reg_acc <- augment(log_fit, new_data = titanic_train) %>%
  accuracy(truth = survived, estimate = .pred_class)


lda_acc <- augment(lda_fit, new_data = titanic_train) %>%
  accuracy(truth = survived, estimate = .pred_class)


qda_acc <- augment(qda_fit, new_data = titanic_train) %>%
  accuracy(truth = survived, estimate = .pred_class)

log_reg_acc
lda_acc
qda_acc
```

## Question 9
$Q = \sum_{i=1}^n (y_i-\hat{y_i})^2$
$\\$
$\hat{y_i}=\beta$
$\\$
$Q = \sum_{i=1}^n (y_i-\beta)^2$
$\leftrightarrow  \frac{d Q}{d \beta} = -2\sum_{i=1}^n (y_i-\beta) = 0$
$\leftrightarrow  \sum_{i=1}^n y_i - n\beta = 0$
$\leftrightarrow  n\beta = \sum_{i=1}^n y_i$
$\leftrightarrow  \hat{\beta} = \frac{\sum_{i=1}^n y_i}{n} = \bar{y}$


## Question 10


















